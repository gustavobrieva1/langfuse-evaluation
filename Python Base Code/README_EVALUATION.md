# AI Response Quality Evaluation System

Sistema de evaluaciÃ³n automÃ¡tica de respuestas de AI usando Gemini 2.0 Flash con arquitectura hÃ­brida.

**ğŸ†• NUEVO:** Ahora soporta **dos modos de ejecuciÃ³n** que se pueden cambiar con un simple parÃ¡metro:
- ğŸ”µ **Gemini API Directa** - Para desarrollo y prototipado (setup simple)
- â˜ï¸ **Vertex AI** - Para producciÃ³n enterprise (seguridad IAM, cuotas altas, monitoreo)

Ver [ğŸ”„ Selector de Evaluadores](#-selector-de-evaluadores) para cambiar entre modos.

---

## ğŸ¯ Objetivo

Evaluar si el AI estÃ¡ respondiendo correctamente segÃºn los documentos consultados, detectando:
- âŒ Alucinaciones (informaciÃ³n inventada)
- âŒ Mezcla incorrecta de informaciÃ³n
- âŒ Respuestas incompletas
- âŒ InformaciÃ³n irrelevante o incoherente

## ğŸ—ï¸ Arquitectura: OpciÃ³n D (HÃ­brida) con Question-Aware Evaluation

```
Input (User Question + Sources + AI Response)
    â†“
Agent 0: Question Quality Evaluator (100% casos)
    â†’ EvalÃºa: claridad, ambigÃ¼edad, completitud de contexto
    â†“
Agent 1: Evaluador Principal (100% casos, question-aware)
    â†’ Ajusta criterios segÃºn calidad de pregunta
    â†’ IF flagged (hallucination major/critical O score < 3):
        â†’ Agent 2: Verificador CrÃ­tico (solo ~18% casos)
    â†“
JSON estructurado final
```

**Ventajas:**
- Costo controlado: +12% vs evaluaciÃ³n simple (por question evaluator adicional)
- PrecisiÃ³n contextual: EvalÃºa respuestas considerando calidad de pregunta
- Velocidad: 82% de casos = 2 llamadas (question + main)
- DetecciÃ³n de preguntas vagas: Identifica cuÃ¡ndo el AI debe pedir clarificaciÃ³n

## ğŸ“Š Criterios Evaluados

### 0. Calidad de la Pregunta (NUEVO)
- **Clarity score (1-5)**: QuÃ© tan especÃ­fica es la pregunta
  - 1 = Extremadamente vaga (ej: "tarjeta de crÃ©dito")
  - 5 = Muy especÃ­fica (ej: "requisitos tarjeta Visa Gold persona natural")
- **Context completeness (1-5)**: Tiene contexto necesario
- **Is ambiguous**: Boolean - pregunta puede interpretarse de mÃºltiples formas
- **Question type**: informational, procedural, comparative, troubleshooting, vague
- **Needs clarification**: Boolean - AI deberÃ­a pedir mÃ¡s informaciÃ³n
- **Missing information**: Lista de datos faltantes
- **Clarification needed**: Lista de preguntas que el AI deberÃ­a hacer

**Impacto en evaluaciÃ³n:**
- Si `clarity_score â‰¤ 2` (pregunta vaga):
  - Es ACEPTABLE que el AI pida clarificaciÃ³n
  - NO se penaliza `completeness` si la pregunta era ambigua
  - Se EVALÃšA si las preguntas clarificadoras del AI son apropiadas
- Si `clarity_score â‰¥ 4` (pregunta especÃ­fica):
  - La respuesta DEBE ser completa y precisa
  - NO es aceptable pedir clarificaciÃ³n innecesariamente

## ğŸ“Š Criterios Evaluados (Response)

### 1. DetecciÃ³n de Alucinaciones (CRÃTICO)
- **Tipos detectados**: URLs, emails, hechos, procedimientos, mezcla de fuentes
- **Severidad**: none, minor, major, critical
- **Output**: Evidencia exacta del texto inventado

### 2. Fidelidad a Fuentes
- **Escala 1-5**: QuÃ© % de la respuesta estÃ¡ soportado por fuentes
- **Grounding ratio**: Ratio de claims soportados vs totales
- **Output**: Lista de claims no soportados

### 3. Completitud
- **Escala 1-5**: Responde todos los aspectos de la pregunta
- **Completeness rate**: % de aspectos respondidos
- **Output**: Aspectos faltantes

### 4. Relevancia
- **Escala 1-5**: Respuesta pertinente (no divaga)
- **Relevance ratio**: % de contenido relevante
- **Output**: Contenido irrelevante detectado

### 5. Coherencia LÃ³gica
- **Escala 1-5**: Sin contradicciones internas
- **Output**: Contradicciones especÃ­ficas detectadas

### 6. EvaluaciÃ³n Global
- **Acceptable**: true/false (decisiÃ³n binaria)
- **Quality tier**: excellent, good, acceptable, poor, critical
- **Overall score**: 1.0-5.0 (promedio ponderado)
- **Recommendation**: approve, review, reject

## ğŸ’° Costos Estimados

Para **1,000 conversaciones** con Gemini 1.5 Flash:

| Componente | Tokens | Costo |
|------------|--------|-------|
| Input (prompts) | ~3.5M | $0.26 |
| Output (JSON) | ~1.0M | $0.30 |
| **TOTAL** | - | **~$0.56** |

- Costo por conversaciÃ³n: **$0.00056**
- Question evaluator: +10% (~130 tokens input, ~120 tokens output por caso)
- VerificaciÃ³n crÃ­tica adicional: +8% (~18% de casos)
- **Total overhead vs evaluaciÃ³n simple: ~12%**

## ğŸš€ InstalaciÃ³n

### 1. Instalar dependencias

```bash
pip install google-generativeai pandas matplotlib seaborn
```

### 2. Obtener API Key de Gemini

1. Ve a [Google AI Studio](https://makersuite.google.com/app/apikey)
2. Crea una API key
3. ConfigÃºrala como variable de entorno:

```bash
export GEMINI_API_KEY='tu-api-key-aqui'
```

O directamente en el notebook (no commitear):
```python
GEMINI_API_KEY = 'tu-api-key-aqui'
```

### 3. Estructura de archivos

```
claude-projects/
â”œâ”€â”€ ai_evaluator.py              # Sistema evaluaciÃ³n (Gemini API)
â”œâ”€â”€ ai_evaluator_vertex.py       # Sistema evaluaciÃ³n (Vertex AI)
â”œâ”€â”€ evaluator_factory.py         # ğŸ†• Selector unificado
â”œâ”€â”€ run_ai_evaluation.ipynb      # Notebook principal
â”œâ”€â”€ test_both_evaluators.py      # ğŸ†• ComparaciÃ³n de evaluadores
â”œâ”€â”€ test_vertex_evaluator.py     # ğŸ†• Test Vertex AI
â”œâ”€â”€ Conecta/
â”‚   â””â”€â”€ langfuse3.csv           # Dataset original
â”œâ”€â”€ README_EVALUATION.md         # Este archivo
â””â”€â”€ VERTEX_AI_SETUP.md          # ğŸ†• GuÃ­a Vertex AI
```

## ğŸ”„ Selector de Evaluadores

**NUEVO:** Ahora puedes cambiar entre Gemini API y Vertex AI con un simple parÃ¡metro.

### OpciÃ³n 1: Variable de Entorno (Recomendado)

```python
# En tu .env file o notebook
import os
from evaluator_factory import create_evaluator

# Configurar modo
os.environ['EVALUATOR_TYPE'] = 'gemini'  # o 'vertex'

# Crear evaluador (el resto del cÃ³digo NO cambia)
evaluator = create_evaluator()
```

### OpciÃ³n 2: ParÃ¡metro ExplÃ­cito

```python
from evaluator_factory import create_evaluator

# Gemini API
evaluator = create_evaluator(
    evaluator_type="gemini",
    gemini_api_key=os.getenv('GEMINI_API_KEY')
)

# O Vertex AI
evaluator = create_evaluator(
    evaluator_type="vertex",
    gcp_project_id=os.getenv('GCP_PROJECT_ID'),
    service_account_key_path=os.getenv('GOOGLE_APPLICATION_CREDENTIALS')
)
```

### OpciÃ³n 3: AutodetecciÃ³n

```python
from evaluator_factory import auto_select_evaluator

# Detecta automÃ¡ticamente segÃºn credenciales disponibles
evaluator = auto_select_evaluator()
```

### ComparaciÃ³n de Evaluadores

| CaracterÃ­stica | Gemini API | Vertex AI |
|----------------|------------|-----------|
| **Setup** | âœ… Muy simple | âš ï¸ Requiere GCP config |
| **AutenticaciÃ³n** | API Key | Service Account (IAM) |
| **Cuotas** | 1,500 RPD (free), 2,000 RPM (paid) | ğŸš€ Mucho mÃ¡s altas |
| **Seguridad** | BÃ¡sica | ğŸ”’ Enterprise (IAM, logging) |
| **Monitoreo** | âŒ No | âœ… Cloud Monitoring |
| **Costos** | Similar | Similar |
| **Uso recomendado** | Desarrollo, prototipado | ğŸ¢ ProducciÃ³n, empresa |

**MigraciÃ³n Gemini â†’ Vertex AI:**
1. Seguir [VERTEX_AI_SETUP.md](VERTEX_AI_SETUP.md)
2. Cambiar `EVALUATOR_TYPE='vertex'` en .env
3. Listo! âœ… (cÃ³digo idÃ©ntico)

### Probar Ambos Evaluadores

```bash
# Configurar credenciales para ambos
export GEMINI_API_KEY="your-key"
export GCP_PROJECT_ID="your-project"
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/key.json"

# Ejecutar comparaciÃ³n
python test_both_evaluators.py
```

---

## ğŸ“– Uso

### EvaluaciÃ³n de Prueba (10 conversaciones)

```python
# En el notebook run_ai_evaluation.ipynb
# Ejecuta hasta la secciÃ³n "4. Run Evaluation"
# Esto evalÃºa solo 10 conversaciones para probar

TEST_SIZE = 10
# ... cÃ³digo ejecuta automÃ¡ticamente
```

### EvaluaciÃ³n Completa

```python
# Descomenta el cÃ³digo en la secciÃ³n "5. Run FULL Evaluation"
# Esto evalÃºa todas las conversaciones del dataset

BATCH_SIZE = 50  # Procesa en lotes
# ... descomenta el bloque completo
```

## ğŸ“ˆ Output

### 1. Archivos CSV

- `evaluation_results_YYYYMMDD_HHMMSS.csv` - Resultados completos
- `critical_hallucinations_YYYYMMDD_HHMMSS.csv` - Solo casos crÃ­ticos

**Columnas principales:**
```
trace_id, session_id,

# Question Quality (NUEVO)
question_clarity_score, question_context_completeness,
question_is_ambiguous, question_type,
question_needs_clarification, question_missing_information,

# Response Evaluation
hallucination_detected, hallucination_severity, hallucination_evidence,
fidelity_score, grounding_ratio,
completeness_score, completeness_rate,
relevance_score, coherence_score,
overall_score, quality_tier, recommendation,
verification_applied, question_aware_adjustment
```

### 2. Reportes JSON

- `evaluation_summary_YYYYMMDD_HHMMSS.json` - EstadÃ­sticas agregadas

```json
{
  "total_conversations": 1000,
  "mean_overall_score": 3.8,
  "acceptable_rate": 0.75,
  "hallucination_rate": 0.12,
  "critical_hallucinations": 15,
  "approval_rate": 0.70,
  "review_rate": 0.22,
  "reject_rate": 0.08,
  "verification_rate": 0.18
}
```

### 3. Visualizaciones

- `evaluation_analysis.png` - 6 grÃ¡ficos de anÃ¡lisis

## ğŸ” AnÃ¡lisis de Resultados

### Identificar Casos CrÃ­ticos

```python
# Casos con alucinaciones graves
critical = results_df[
    (results_df['hallucination_severity'].isin(['major', 'critical']))
]

# Casos de baja calidad
low_quality = results_df[results_df['overall_score'] < 2.5]

# Casos que necesitan revisiÃ³n
to_review = results_df[results_df['recommendation'] == 'review']
```

### EstadÃ­sticas por Criterio

```python
print(f"Mean Fidelity Score: {results_df['fidelity_score'].mean():.2f}")
print(f"Hallucination Rate: {results_df['hallucination_detected'].mean():.2%}")
print(f"Approval Rate: {(results_df['recommendation']=='approve').mean():.2%}")
```

### DistribuciÃ³n de Problemas

```python
# Tipos de alucinaciones mÃ¡s comunes
results_df['hallucination_types'].value_counts()

# Severidad de problemas
results_df['hallucination_severity'].value_counts()
```

### AnÃ¡lisis de Calidad de Preguntas (NUEVO)

```python
# DistribuciÃ³n de claridad de preguntas
print(f"Mean Clarity Score: {results_df['question_clarity_score'].mean():.2f}")
print(results_df['question_type'].value_counts())

# Preguntas vagas que necesitan clarificaciÃ³n
vague_questions = results_df[results_df['question_clarity_score'] <= 2]
print(f"Vague Questions: {len(vague_questions)} ({len(vague_questions)/len(results_df)*100:.1f}%)")

# CorrelaciÃ³n entre claridad de pregunta y calidad de respuesta
correlation = results_df['question_clarity_score'].corr(results_df['overall_score'])
print(f"Question Clarity â†” Overall Score: {correlation:.3f}")

# Casos donde preguntas vagas llevaron a respuestas incompletas
vague_incomplete = results_df[
    (results_df['question_clarity_score'] <= 2) &
    (results_df['completeness_score'] < 3)
]
```

### Ejemplos Detallados de EvaluaciÃ³n (NUEVO)

El notebook incluye funciones para inspeccionar el razonamiento completo del agente evaluador:

```python
# Ver evaluaciÃ³n detallada de un caso especÃ­fico
display_evaluation_example(analysis_df, 0)  # Primer caso
display_evaluation_example(analysis_df, 5, show_sources=True)  # Con fuentes

# Comparar dos casos lado a lado
compare_evaluations(analysis_df, 0, 3)  # Compara caso 1 vs caso 4
```

**QuÃ© muestra `display_evaluation_example()`:**
- ğŸ“ Pregunta original del usuario
- ğŸ” EvaluaciÃ³n de calidad de la pregunta (clarity, ambigÃ¼edad, missing info)
- ğŸ¤– Respuesta del AI
- ğŸ“š Fuentes consultadas (opcional)
- âš–ï¸ EvaluaciÃ³n completa de la respuesta:
  - Overall score y quality tier
  - DetecciÃ³n de alucinaciones con evidencia
  - Scores individuales (fidelity, completeness, relevance, coherence)
  - AnÃ¡lisis de claims (soportados vs no soportados)
  - Aspectos faltantes
  - VerificaciÃ³n crÃ­tica (si aplicÃ³)
  - Ajustes por calidad de pregunta
- ğŸ’­ Razonamiento final del evaluador

**Ejemplos automÃ¡ticos:**
El notebook tambiÃ©n genera automÃ¡ticamente ejemplos de:
- âœ… Mejor caso (highest score)
- âŒ Peor caso (lowest score)
- ğŸš¨ Caso con alucinaciÃ³n
- âš ï¸ Caso con pregunta vaga
- âœ… Caso con pregunta especÃ­fica

Esto permite inspeccionar cÃ³mo el agente estÃ¡ razonando y validar que estÃ¡ evaluando correctamente.

## âš™ï¸ PersonalizaciÃ³n

### Ajustar criterios de verificaciÃ³n

En `ai_evaluator.py`, lÃ­nea ~380:

```python
needs_verification = (
    result_dict['hallucination_check']['detected'] and
    result_dict['hallucination_check']['severity'] in ['major', 'critical']
) or (
    result_dict['overall_quality']['overall_score'] < 3.0
)
```

Cambia los umbrales segÃºn tus necesidades:
- `< 3.0` â†’ `< 2.5` (menos verificaciones)
- `< 3.0` â†’ `< 3.5` (mÃ¡s verificaciones)

### Ajustar ponderaciÃ³n de scores

En el prompt (ai_evaluator.py, lÃ­nea ~220):

```python
OVERALL SCORE: promedio ponderado
- fidelity_score Ã— 0.35    # Peso de fidelidad
- completeness Ã— 0.25       # Peso de completitud
- relevance Ã— 0.20          # Peso de relevancia
- coherence Ã— 0.20          # Peso de coherencia
```

## ğŸ› Troubleshooting

### Error: "API key not configured"

```bash
export GEMINI_API_KEY='your-key-here'
```

### Error: "JSON Parse Error"

El modelo a veces devuelve markdown. El parser lo maneja automÃ¡ticamente, pero si falla:
1. Revisa el `response_text` en el error
2. Ajusta la temperatura en `generation_config` (lÃ­nea 48)

### Error: "Rate limit exceeded"

Gemini Flash 2.0 tiene rate limits:
- **Free tier**: 15 RPM (requests per minute)
- **Paid tier**: 1000 RPM

SoluciÃ³n:
```python
import time
# Agregar delay entre requests
time.sleep(1)  # 1 segundo entre llamadas
```

### Costo demasiado alto

1. EvalÃºa muestra mÃ¡s pequeÃ±a primero
2. Ajusta umbral de verificaciÃ³n para reducir casos verificados
3. Usa batches mÃ¡s pequeÃ±os

## ğŸ“š Referencias

- [Gemini API Docs](https://ai.google.dev/docs)
- [Gemini Pricing](https://ai.google.dev/pricing)
- [Python SDK](https://github.com/google/generative-ai-python)

## ğŸ¤ Contribuciones

Para mejorar el sistema:

1. **Mejorar prompts**: Los prompts estÃ¡n en `ai_evaluator.py`
2. **Agregar mÃ©tricas**: Modificar `EvaluationResult` dataclass
3. **Nuevos anÃ¡lisis**: Agregar celdas en el notebook

## ğŸ“ Changelog

### v1.0 (2025-01-XX)
- Sistema inicial con arquitectura hÃ­brida
- EvaluaciÃ³n de 5 criterios principales
- DetecciÃ³n de alucinaciones con verificaciÃ³n
- Export a CSV y JSON
- Visualizaciones automÃ¡ticas
