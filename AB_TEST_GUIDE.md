# A/B Testing: Prompt V1 (Lenient) vs V2 (Strict)

## Executive Summary

Implementamos A/B testing para comparar dos versiones del prompt de detecci√≥n de alucinaciones:

- **V1 (Lenient)**: Enfoque original con criterios subjetivos
- **V2 (Strict)**: Enfoque estricto con √°rbol de decisi√≥n binario y penalidades severas

**Objetivo:** Detectar con mayor precisi√≥n cuando Conecta inventa informaci√≥n o da informaci√≥n err√≥nea.

---

## Diferencias Clave

### 1. **Filosof√≠a**

| Aspecto | V1 (Lenient) | V2 (Strict) |
|---------|--------------|-------------|
| Tolerancia | "When in doubt, flag it" (ambiguo) | "ZERO TOLERANCE - false positives acceptable, false negatives NOT" |
| Enfoque | Subjetivo, basado en interpretaci√≥n | Objetivo, √°rbol de decisi√≥n binario |
| Sector | Gen√©rico | Espec√≠fico bancario (compliance cr√≠tico) |

### 2. **Proceso de Evaluaci√≥n**

**V1 (Lenient):**
```
1. Extract claims
2. Search for support in docs
3. Flag if "not clearly supported" ‚Üê Subjetivo
4. Assign severity based on "harm" ‚Üê Subjetivo
```

**V2 (Strict):**
```
STEP 1: EXACT MATCH
‚Üí Quote or paraphrase exists?
   YES ‚Üí Grounded ‚úÖ
   NO ‚Üí Go to STEP 2

STEP 2: ENTITY CHECK
‚Üí Same entity (product/service)?
   Doc="Fondos", Claim="Fondos" ‚Üí Same, go to STEP 3
   Doc="Fondos", Claim="Fiducia" ‚Üí Different, HALLUCINATION ‚ùå

STEP 3: INFERENCE VALIDITY
‚Üí Can be safely inferred from same entity?
   Combining Doc1+Doc2 about Product A ‚Üí Valid ‚úÖ
   Applying Product A info to Product B ‚Üí Invalid ‚ùå
```

### 3. **Severity Assignment**

**V1 (Lenient):**
```
CRITICAL: "could harm customer" ‚Üê ¬øQu√© es harm?
MAJOR: "misleads but won't cause immediate harm" ‚Üê ¬øCu√°ndo es immediate?
MINOR: "small details incorrect" ‚Üê ¬øQu√© es small?
```

**V2 (Strict) - Reglas Objetivas:**
```
CRITICAL (severity=3) - MAXIMUM PENALTY:
‚îú‚îÄ Amounts (interest rates, fees, minimums)
‚îú‚îÄ Contact info (phone, email, branches)
‚îú‚îÄ Legal/compliance procedures
‚îú‚îÄ Deadlines, timeframes
‚îî‚îÄ Account numbers, IDs, codes

MAJOR (severity=2):
‚îú‚îÄ Product features misrepresented
‚îú‚îÄ Process steps incorrect
‚îî‚îÄ Requirements fabricated

MINOR (severity=1):
‚îú‚îÄ Product name variations (semantic same)
‚îú‚îÄ Formatting differences
‚îî‚îÄ Non-critical details
```

### 4. **Casos Espec√≠ficos**

#### Ejemplo 1: "Fiducia Estructurada"

**Pregunta:** "como cancelo una fiducia estructurada?"
**Conecta:** "Para cancelar una fiducia estructurada, llama al 018000..."
**Documents:** Solo mencionan "Fondos de Inversi√≥n" y "Dafuturo"

**V1 (Lenient) - Comportamiento actual:**
```json
{
  "hallucination_detected": false,  ‚ùå INCONSISTENTE
  "severity": "none",
  "evidence": [{"status": "grounded"}],  ‚Üê Todo marcado como grounded
  "assessment": "mixing hallucination detected"  ‚Üê Pero aqu√≠ s√≠ lo detecta
}
```

**V2 (Strict) - Comportamiento esperado:**
```json
{
  "hallucination_detected": true,  ‚úÖ CONSISTENTE
  "severity": "critical",  ‚Üê Contact info = CRITICAL
  "hallucination_type": "entity_substitution",
  "evidence": [{
    "claim": "Para cancelar fiducia estructurada, llama al 018000...",
    "status": "hallucination",  ‚Üê Marcado correctamente
    "step_failed": 2,  ‚Üê Fall√≥ en ENTITY CHECK
    "severity": 3,
    "document_support": "NOT FOUND: Documents only cover 'Fondos' and 'Dafuturo', NOT 'fiducia estructurada'"
  }]
}
```

#### Ejemplo 2: Inferencia V√°lida

**Pregunta:** "Qu√© requisitos tiene y cu√°nto cuesta el producto X?"
**Documents:** Doc1 = "Producto X requiere c√©dula", Doc2 = "Producto X cuesta $100"
**Conecta:** "Producto X requiere c√©dula y cuesta $100"

**V1:** Podr√≠a marcar como "mixing" (combinando docs)
**V2:** GROUNDED ‚úÖ (STEP 3 - Valid inference, same entity)

#### Ejemplo 3: Sustituci√≥n Inv√°lida

**Pregunta:** "Qu√© requisitos tiene el producto Y?"
**Documents:** Solo "Producto X requiere c√©dula"
**Conecta:** "Producto Y requiere c√©dula"

**V1:** Podr√≠a marcar como "grounded" (requisito est√° en docs)
**V2:** HALLUCINATION ‚ùå (STEP 2 - Entity substitution)

---

## Impacto Esperado

### Tasa de Detecci√≥n

**V1 (Lenient):**
- Detecta: 15-25% de conversaciones
- Principalmente: Fabricaciones obvias
- Pierde: Entity substitution, mixing sutil

**V2 (Strict):**
- Detecta: 30-50% de conversaciones (‚Üë 20-30%)
- Detecta: Todo lo de V1 + entity substitution + mixing
- Severidad: M√°s casos CRITICAL (contact info, amounts)

### Consistencia Estructural

**V1:**
- Inconsistencias entre `assessment` y `evidence` array
- Requiere validaci√≥n post-procesamiento

**V2:**
- Reglas expl√≠citas de consistencia:
  - `hallucinated_count > 0` ‚Üí `detected = true`
  - `assessment` debe coincidir con `evidence`
- Campo `step_failed` indica d√≥nde fall√≥

### Casos Cr√≠ticos

**V1:** Contact info err√≥nea = MAJOR o MINOR (inconsistente)
**V2:** Contact info err√≥nea = CRITICAL (siempre)

---

## Riesgos y Mitigaciones

### Riesgo 1: Falsos Positivos

**Riesgo:** V2 es tan estricto que marca casos leg√≠timos

**Mitigaci√≥n:**
- A/B test en 50+ conversaciones
- Manual review de casos "V2 only"
- Target: <10% false positive rate

**Criterio de aceptaci√≥n:**
```python
false_positive_rate = cases_v2_only_invalid / cases_v2_only_total
if false_positive_rate < 0.10:
    # V2 is acceptable
elif false_positive_rate < 0.20:
    # Tune V2 (adjust entity matching rules)
else:
    # Keep V1, V2 too strict
```

### Riesgo 2: Sobrecarga de Casos

**Riesgo:** 2x m√°s hallucinations detectadas ‚Üí m√°s trabajo de revisi√≥n

**Mitigaci√≥n:**
- Priorizar por severity: CRITICAL > MAJOR > MINOR
- Automatizar correcciones para patterns comunes
- Dashboard para tracking

### Riesgo 3: Performance

**Riesgo:** Prompt V2 es m√°s largo ‚Üí m√°s tokens ‚Üí m√°s costo/latencia

**Mitigaci√≥n:**
- V2 prompt: ~2K tokens (vs V1: ~800 tokens)
- Cost increase: ~$1.20/1K conversations (de $3.70 a $4.90)
- Aceptable para mejora en detecci√≥n

---

## C√≥mo Ejecutar el A/B Test

### Paso 1: Ejecutar Test

```bash
# Default: 20 conversations
python ab_test_prompts.py

# Custom sample size
python ab_test_prompts.py --limit 50

# Custom output
python ab_test_prompts.py --limit 100 --output-dir ./results
```

### Paso 2: Analizar Resultados

El script imprime autom√°ticamente:
```
A/B TEST SUMMARY
================

Conversations tested: 50

üìä V1 (LENIENT PROMPT):
   Success rate: 100.0%
   Hallucination rate: 25.0% (12/50)
   Severity breakdown:
      minor: 8
      major: 3
      critical: 1

üìä V2 (STRICT PROMPT):
   Success rate: 100.0%
   Hallucination rate: 38.0% (19/50)
   Severity breakdown:
      minor: 9
      major: 5
      critical: 5

üîç COMPARISON:
   Hallucination rate difference: +13.0%
   ‚Üí V2 is moderately stricter (+13.0%)
```

### Paso 3: Manual Review

```python
# Load results
df_v1 = pd.read_csv('ab_test_results/ab_test_v1_lenient_TIMESTAMP.csv')
df_v2 = pd.read_csv('ab_test_results/ab_test_v2_strict_TIMESTAMP.csv')

# Cases where V2 detected but V1 didn't
merged = df_v1.merge(df_v2, on='session_id', suffixes=('_v1', '_v2'))
v2_only = merged[
    (merged['hall_hallucination_detected_v1'] == False) &
    (merged['hall_hallucination_detected_v2'] == True)
]

print(f"Cases to manually review: {len(v2_only)}")

# Review critical cases first
critical_v2_only = v2_only[v2_only['hall_severity_v2'] == 'critical']
print(f"\nCritical cases (review first): {len(critical_v2_only)}")
```

### Paso 4: Decidir

**Usar V2 (Strict) si:**
- False positive rate < 10%
- Detecta ‚â•80% de critical cases de V1
- Detecta 20-50% m√°s hallucinations totales

**Mantener V1 (Lenient) si:**
- False positive rate > 20%
- V2 pierde critical cases de V1
- V2 detecta >80% m√°s (demasiado estricto)

**Ajustar V2 si:**
- False positive rate 10-20%
- Demasiados MINOR escalados a MAJOR

---

## Implementaci√≥n de Cambios

### Usar V2 por Default

Edita `src/config.py`:

```python
@dataclass
class EvaluatorConfig:
    # ...
    # A/B Testing
    prompt_version: str = "v2"  # Changed from "v1"
```

### Uso Selectivo

```python
# V1 para an√°lisis exploratorio
config_v1 = EvaluatorConfig(prompt_version="v1")
orchestrator_v1 = EvaluationOrchestrator(config_v1)

# V2 para detecci√≥n cr√≠tica
config_v2 = EvaluatorConfig(prompt_version="v2")
orchestrator_v2 = EvaluationOrchestrator(config_v2)
```

---

## Ejemplo de Output Comparado

### Caso: "Fiducia Estructurada"

#### V1 Output (Inconsistente):
```
üö® HALLUCINATION CHECK:
   Detected: False  ‚Üê WRONG
   Severity: none
   Grounding Ratio: 100.00%  ‚Üê WRONG
   Assessment: "mixing hallucination detected"  ‚Üê RIGHT (but ignored)
```

#### V2 Output (Consistente):
```
üö® HALLUCINATION CHECK:
   Detected: True  ‚Üê CORRECT
   Severity: critical  ‚Üê CORRECT (contact info)
   Grounding Ratio: 0.00%  ‚Üê CORRECT
   Step Failed: 2 (Entity Check)
   Assessment: "Entity substitution - applying info from 'Fondos' to 'Fiducia' without evidence"
```

---

## Recursos

- **Script de testing:** `ab_test_prompts.py`
- **Gu√≠a de an√°lisis:** `notebooks/ab_test_analysis.md`
- **C√≥digo fuente V1:** `src/utils/prompt_templates.py::_hallucination_detector_v1()`
- **C√≥digo fuente V2:** `src/utils/prompt_templates.py::_hallucination_detector_v2()`

---

## M√©tricas de √âxito

### Objetivo Principal
Reducir hallucinations de 25% ‚Üí <15% en 6 meses

### M√©tricas A/B Test
- V2 detecta ‚â•90% de critical cases de V1
- V2 detecta 20-40% m√°s total hallucinations
- False positive rate <10%
- Consistencia estructural: 100% (no m√°s assessment vs evidence mismatch)

### M√©tricas Post-Implementaci√≥n
- Critical hallucinations detectadas: 100% (vs ~60% actual)
- Tiempo de revisi√≥n manual: -30% (mejor clasificaci√≥n por severity)
- Compliance risk: Reducido (contact info errors = CRITICAL)
